{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f125b8c1-fa7a-4933-ad29-15366336db8b",
   "metadata": {},
   "source": [
    "## Fine-tuning TinyLlama using Ultrachat dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edaa0e91-6084-46af-baf4-b9166f063523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf8d635-b3f7-4e2c-81f9-3a45200345d6",
   "metadata": {},
   "source": [
    "#### Lets load & explore the Ultrachat dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88541cef-5447-42c6-83aa-7efcadbfb129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small subset of Ultrachat dataset\n",
    "dataset = (\n",
    "    load_dataset(\"HuggingFaceH4/ultrachat_200k\",  split=\"test_sft\")\n",
    "      .shuffle(seed=42)\n",
    "      .select(range(3_000))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "630bbe2b-a671-42f8-8aeb-e463ba226a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'prompt_id', 'messages'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ca870b3-2c2e-40e1-b2b6-bbd327e6ba76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Does the University of Pennsylvania offer any programs for non-traditional students?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['prompt'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfd1b479-6bba-4d4f-b806-88fd5a842702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Does the University of Pennsylvania offer any programs for non-traditional students?',\n",
       "  'role': 'user'},\n",
       " {'content': 'Yes, the University of Pennsylvania offers several programs for non-traditional students, including:\\n\\n1. Penn LPS Online: This program offers online courses and degree programs for non-traditional students who wish to earn a degree from Penn. It offers several undergraduate and graduate degree programs.\\n\\n2. College of Liberal and Professional Studies: This program offers a variety of degree programs, including full-time, part-time, online, and on-campus options. It is designed for working professionals and those who wish to complete their degree later in life.\\n\\n3. Executive Education: This program offers short-term, intensive courses for working professionals who wish to enhance their skills and knowledge in their field.\\n\\n4. Summer Sessions: This program offers summer courses for undergraduate and graduate students, including non-traditional students who wish to complete their degree faster.\\n\\n5. Continuing Education: This program offers non-credit courses and workshops for professionals and individuals who wish to enhance their skills and knowledge in specific areas.',\n",
       "  'role': 'assistant'},\n",
       " {'content': \"Oh, that's great! Which program would you recommend for someone who wants to study part-time while working?\",\n",
       "  'role': 'user'},\n",
       " {'content': \"If you're looking to study part-time while working, I would recommend the College of Liberal and Professional Studies (LPS). LPS offers part-time options for undergraduate and graduate programs, as well as flexible schedules that can accommodate the needs of working adults. They also have online and on-campus courses available, depending on your preferences. Additionally, LPS has a variety of certificate programs that allow you to take courses in a specific area without committing to a full degree program. This can be a great option if you want to enhance your skills and knowledge in a specific area while working.\",\n",
       "  'role': 'assistant'},\n",
       " {'content': \"That sounds perfect! Can you tell me more about the certificate programs offered by LPS? I'm interested in gaining some new skills without committing to a full degree program.\",\n",
       "  'role': 'user'},\n",
       " {'content': 'Sure! The College of Liberal and Professional Studies (LPS) offers a variety of certificate programs in different fields. These programs are designed to provide focused coursework and training in a specific area, without the commitment of a full degree program. \\n\\nSome of the certificate programs offered by LPS include:\\n \\n1. Advanced Analytics: This program covers advanced data analysis techniques and principles, and prepares graduates for a career in data analytics.\\n\\n2. Business and Public Policy: This program covers the intersection of business and public policy, and prepares graduates to work in policy-making or public service.\\n\\n3. Nonprofit Administration: This program covers the fundamentals of nonprofit management, and prepares graduates to lead and manage nonprofit organizations.\\n\\n4. Organizational Dynamics: This program focuses on organizational behavior, and prepares graduates to manage people and organizations effectively.\\n\\n5. Project Management: This program covers project management principles and techniques, and prepares graduates for a career in project management.\\n\\nThese are just a few examples of the many certificate programs offered by LPS. Each program has its own specific requirements and coursework, so I would recommend checking out the LPS website for more information on each program.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['messages'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c68c2-8541-4908-9b48-1510de709571",
   "metadata": {},
   "source": [
    "#### Convert this instruction dataset into a chat template dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11330aa4-7f10-43c9-b3c1-934ed78d1d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction dataset is typically prepared as a chat template.\n",
    "# We can use the same template as the chat version of TinyLlama\n",
    "\n",
    "template_tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f94e2d7c-cb4d-4eab-9d23-396a369482ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple function to format the dataset into a chat template used by TinyLlama\n",
    "def format_prompt(example):\n",
    "    \"\"\"Format the prompt to using the <|user|> template TinyLLama is using\"\"\"\n",
    "\n",
    "    # Format answers\n",
    "    chat = example[\"messages\"]\n",
    "    prompt = template_tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ce3a730-464e-4470-9138-fed995a57f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(format_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc9bad0d-d682-4b51-9987-c37e64f78386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'prompt_id', 'messages', 'text'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3d52226-c905-42b2-93c2-c01105419781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Does the University of Pennsylvania offer any programs for non-traditional students?</s>\n",
      "<|assistant|>\n",
      "Yes, the University of Pennsylvania offers several programs for non-traditional students, including:\n",
      "\n",
      "1. Penn LPS Online: This program offers online courses and degree programs for non-traditional students who wish to earn a degree from Penn. It offers several undergraduate and graduate degree programs.\n",
      "\n",
      "2. College of Liberal and Professional Studies: This program offers a variety of degree programs, including full-time, part-time, online, and on-campus options. It is designed for working professionals and those who wish to complete their degree later in life.\n",
      "\n",
      "3. Executive Education: This program offers short-term, intensive courses for working professionals who wish to enhance their skills and knowledge in their field.\n",
      "\n",
      "4. Summer Sessions: This program offers summer courses for undergraduate and graduate students, including non-traditional students who wish to complete their degree faster.\n",
      "\n",
      "5. Continuing Education: This program offers non-credit courses and workshops for professionals and individuals who wish to enhance their skills and knowledge in specific areas.</s>\n",
      "<|user|>\n",
      "Oh, that's great! Which program would you recommend for someone who wants to study part-time while working?</s>\n",
      "<|assistant|>\n",
      "If you're looking to study part-time while working, I would recommend the College of Liberal and Professional Studies (LPS). LPS offers part-time options for undergraduate and graduate programs, as well as flexible schedules that can accommodate the needs of working adults. They also have online and on-campus courses available, depending on your preferences. Additionally, LPS has a variety of certificate programs that allow you to take courses in a specific area without committing to a full degree program. This can be a great option if you want to enhance your skills and knowledge in a specific area while working.</s>\n",
      "<|user|>\n",
      "That sounds perfect! Can you tell me more about the certificate programs offered by LPS? I'm interested in gaining some new skills without committing to a full degree program.</s>\n",
      "<|assistant|>\n",
      "Sure! The College of Liberal and Professional Studies (LPS) offers a variety of certificate programs in different fields. These programs are designed to provide focused coursework and training in a specific area, without the commitment of a full degree program. \n",
      "\n",
      "Some of the certificate programs offered by LPS include:\n",
      " \n",
      "1. Advanced Analytics: This program covers advanced data analysis techniques and principles, and prepares graduates for a career in data analytics.\n",
      "\n",
      "2. Business and Public Policy: This program covers the intersection of business and public policy, and prepares graduates to work in policy-making or public service.\n",
      "\n",
      "3. Nonprofit Administration: This program covers the fundamentals of nonprofit management, and prepares graduates to lead and manage nonprofit organizations.\n",
      "\n",
      "4. Organizational Dynamics: This program focuses on organizational behavior, and prepares graduates to manage people and organizations effectively.\n",
      "\n",
      "5. Project Management: This program covers project management principles and techniques, and prepares graduates for a career in project management.\n",
      "\n",
      "These are just a few examples of the many certificate programs offered by LPS. Each program has its own specific requirements and coursework, so I would recommend checking out the LPS website for more information on each program.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (dataset['text'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfbae5f-8156-491f-a561-32d219a2accc",
   "metadata": {},
   "source": [
    "#### Load the actual tiny llama base model so that we can fine tune it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ca5d0b9-ca88-4491-be73-eb1d418574e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization configuration - Q in QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Use 4-bit precision model loading\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Quantization type\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # Compute dtype\n",
    "    bnb_4bit_use_double_quant=True,  # Apply nested quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "591fe485-2800-4d8a-8219-8bfae158817a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70de7f6383b4533b421d05ea6fdfc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sridh\\anaconda3\\envs\\llms_clone\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sridh\\.cache\\huggingface\\hub\\models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the model to train on the GPU\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Leave this out for regular SFT\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llms_clone\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llms_clone\\Lib\\site-packages\\transformers\\modeling_utils.py:3452\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3449\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3452\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[0;32m   3454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3455\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[0;32m   3456\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llms_clone\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:82\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n\u001b[0;32m     81\u001b[0m bnb_multibackend_is_enabled \u001b[38;5;241m=\u001b[39m is_bitsandbytes_multi_backend_available()\n\u001b[1;32m---> 82\u001b[0m \u001b[43mvalidate_bnb_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_flax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sure the weights are in PyTorch format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     88\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llms_clone\\Lib\\site-packages\\transformers\\integrations\\bitsandbytes.py:558\u001b[0m, in \u001b[0;36mvalidate_bnb_backend_availability\u001b[1;34m(raise_exception)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bitsandbytes_multi_backend_available():\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _validate_bnb_multi_backend_availability(raise_exception)\n\u001b[1;32m--> 558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_validate_bnb_cuda_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llms_clone\\Lib\\site-packages\\transformers\\integrations\\bitsandbytes.py:536\u001b[0m, in \u001b[0;36m_validate_bnb_cuda_backend_availability\u001b[1;34m(raise_exception)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_exception:\n\u001b[0;32m    535\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(log_msg)\n\u001b[1;32m--> 536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(log_msg)\n\u001b[0;32m    538\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(log_msg)\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend"
     ]
    }
   ],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "# Load the model to train on the GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "\n",
    "    # Leave this out for regular SFT\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341cc7e-33f0-4cb2-bc1b-a8f54c8984ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beae9219-e2f0-4a3c-876c-fd384027c002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n",
      "CUDA Device Count: 0\n",
      "CUDA Version: None\n",
      "PyTorch Version: 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"PyTorch Version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ebb14-2679-4882-9b66-6ed6a7541dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
